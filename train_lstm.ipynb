{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from lib.ekyn import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from lib.ekyn import get_dataloaders,get_ekyn_ids\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Dumbledore(nn.Module):\n",
    "    def __init__(self,encoder_experiment_name,sequence_length,hidden_size=16,num_layers=1,dropout=None,frozen_encoder=True) -> None:\n",
    "        super().__init__()\n",
    "        self.frozen = frozen_encoder\n",
    "        self.sequence_length = sequence_length\n",
    "        self.encoder = self.get_encoder(encoder_experiment_name)\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.classifier = nn.Linear(in_features=hidden_size,out_features=3)\n",
    "    def forward(self,x):\n",
    "        x = x.flatten(0,1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.reshape(-1,self.sequence_length,3)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        x = nn.functional.relu(output[:,-1])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    def get_encoder(self,encoder_experiment_name):\n",
    "        state = torch.load(f'experiments/{encoder_experiment_name}/state.pt',map_location='cpu')\n",
    "        encoder = copy.deepcopy(state['model'])\n",
    "        encoder.load_state_dict(state['best_model_wts'])\n",
    "        if self.frozen:\n",
    "            print(\"Model is freezing encoder\")\n",
    "            for p in encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        return encoder\n",
    "class SequencedDatasetv2(Dataset):\n",
    "    def __init__(self, id, condition, sequence_length, stride=1):\n",
    "        self.sequence_length = sequence_length\n",
    "        # X, y = load_ekyn_pt_robust(id=id, condition=condition, downsampled=True)\n",
    "        X,y = load_ekyn_pt(id=id,condition=condition)\n",
    "        \n",
    "        # Assuming X.shape is (num_samples, num_features) and y.shape is (num_samples, num_classes)\n",
    "        num_features = X.shape[1]\n",
    "        num_classes = y.shape[1]\n",
    "        \n",
    "        # Pad the sequence\n",
    "        self.X = torch.cat([torch.zeros(sequence_length // 2, num_features), X, torch.zeros(sequence_length // 2, num_features)]).unsqueeze(1)\n",
    "        self.y = torch.cat([torch.zeros(sequence_length // 2, num_classes), y, torch.zeros(sequence_length // 2, num_classes)])\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        for i in range(0, len(self.y) - sequence_length, stride):\n",
    "            self.sequences.append(self.X[i:i + sequence_length])\n",
    "            self.labels.append(self.y[i + sequence_length // 2])\n",
    "        \n",
    "        self.sequences = torch.stack(self.sequences)\n",
    "        self.labels = torch.stack(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "def get_dataloaders(batch_size=512,shuffle_train=True,shuffle_test=False,sequence_length=3,stride=1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    ekyn_ids = get_ekyn_ids()\n",
    "\n",
    "    train_ids,test_ids = train_test_split(ekyn_ids,test_size=.2,shuffle=True,random_state=0)\n",
    "\n",
    "    from torch.utils.data import DataLoader,ConcatDataset\n",
    "    trainloader = DataLoader(\n",
    "            dataset=ConcatDataset(\n",
    "            [SequencedDatasetv2(id=id,condition=condition,sequence_length=sequence_length,stride=stride) for id in train_ids for condition in ['Vehicle','PF']] \n",
    "            ),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_train,\n",
    "            num_workers=1\n",
    "        )\n",
    "    testloader = DataLoader(\n",
    "            dataset=ConcatDataset(\n",
    "            [SequencedDatasetv2(id=id,condition=condition,sequence_length=sequence_length,stride=1) for id in test_ids for condition in ['Vehicle','PF']] \n",
    "            ),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_test,\n",
    "            num_workers=1\n",
    "        )\n",
    "    return trainloader,testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.utils import count_params\n",
    "sequence_length = 9\n",
    "trainloader,testloader = get_dataloaders(batch_size=512,sequence_length=sequence_length,stride=50)\n",
    "model = Dumbledore(encoder_experiment_name=f'2024_14_08_16_04_12',sequence_length=sequence_length,hidden_size=64,num_layers=1,dropout=.1,frozen_encoder=True)\n",
    "print(count_params(model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=3e-2,weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlossi = []\n",
    "testlossi = []\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_dev_loss = torch.inf\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "import matplotlib.pyplot as plt\n",
    "for i in tqdm(range(100)):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    for Xi,yi in trainloader:\n",
    "        Xi,yi = Xi.to('cuda'),yi.to('cuda')\n",
    "        logits = model(Xi)\n",
    "        loss = criterion(logits,yi)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "    trainlossi.append(loss_total/len(trainloader))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_total = 0\n",
    "        for Xi,yi in testloader:\n",
    "            Xi,yi = Xi.to('cuda'),yi.to('cuda')\n",
    "            logits = model(Xi)\n",
    "            loss = criterion(logits,yi)\n",
    "            loss_total += loss.item()\n",
    "        testlossi.append(loss_total/len(testloader))\n",
    "        \n",
    "    scheduler.step(loss_total/len(testloader))\n",
    "    if testlossi[-1] < best_dev_loss:\n",
    "        best_dev_loss = testlossi[-1]\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.plot(trainlossi[2:])\n",
    "    plt.plot(testlossi[2:])\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('loss.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = copy.deepcopy(model)\n",
    "best_model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.utils import *\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(8,4))\n",
    "trainloader,testloader = get_dataloaders(batch_size=512,sequence_length=sequence_length,stride=10)\n",
    "loss,y_true,y_pred = evaluate(dataloader=trainloader,model=model,criterion=criterion,device='cuda')\n",
    "print(loss)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,y_pred,normalize='true',ax=axes[0],colorbar=False)\n",
    "axes[0].set_title(f'trainf1 : {f1_score(y_true,y_pred,average=\"macro\"):.3f}')\n",
    "loss,y_true,y_pred = evaluate(dataloader=testloader,model=model,criterion=criterion,device='cuda')\n",
    "print(loss)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,y_pred,normalize='true',ax=axes[1],colorbar=False)\n",
    "axes[1].set_title(f'testf1 : {f1_score(y_true,y_pred,average=\"macro\"):.3f}')\n",
    "# train .886 test .858 seq 3\n",
    "# train .897 test .869 seq 5\n",
    "# train .906 test .872 seq 9\n",
    "# train .903 test .872 seq 9 hidden 16 dropout .2 layers 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.utils import *\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "stride_vs_f1 = {\n",
    "\n",
    "}\n",
    "for stride in range(10,20):\n",
    "    trainloader,testloader = get_dataloaders(batch_size=512,sequence_length=sequence_length,stride=stride)\n",
    "    loss,y_true,y_pred = evaluate(dataloader=testloader,model=model,criterion=criterion,device='cuda')\n",
    "    print(loss)\n",
    "    stride_vs_f1[stride] = f1_score(y_true,y_pred,average=\"macro\")\n",
    "# train .886 test .858 seq 3\n",
    "# train .897 test .869 seq 5\n",
    "# train .906 test .872 seq 9\n",
    "# train .903 test .872 seq 9 hidden 16 dropout .2 layers 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(stride_vs_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
