{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# TODO : change names back to integers?\n",
    "from lib.ekyn import *\n",
    "from sage.utils import *\n",
    "from sage.models import *\n",
    "from lib.env import *\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "hyperparameters = {\n",
    "    'experiment_group_id':'encoder',\n",
    "    'weight_decay':1e-2,\n",
    "    'lr':3e-4,\n",
    "    'batch_size':512,\n",
    "    'robust':True,\n",
    "    'norm':'layer',\n",
    "    'dropout':.1,\n",
    "    'stem_kernel_size':3,\n",
    "    'widthi':[64],\n",
    "    'depthi':[2],\n",
    "    'n_output_neurons':3,\n",
    "    'patience':100,\n",
    "    'epochs':500,\n",
    "    'device':f'cuda',\n",
    "    'dataloaders':'leave_one_out',\n",
    "    'dev_set':True,\n",
    "    'fold':0\n",
    "}\n",
    "\n",
    "dataloaders = get_dataloaders(**hyperparameters)\n",
    "model = ResNetv2(block=ResBlockv2,**hyperparameters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),lr=hyperparameters['lr'],weight_decay=hyperparameters['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)\n",
    "\n",
    "state = {\n",
    "    'epoch':0,\n",
    "    'start_time':datetime.datetime.now().strftime(\"%Y_%d_%m_%H_%M_%S\"),\n",
    "    'execution_time':0,\n",
    "    'trainlossi':[],\n",
    "    'devlossi':[],\n",
    "    'devf1i':[],\n",
    "    'testlossi':[],\n",
    "    'testf1i':[],\n",
    "    'best_dev_loss':torch.inf,\n",
    "    'best_test_loss':torch.inf,\n",
    "    'model':model,\n",
    "    'scheduler':scheduler,\n",
    "    'criterion':criterion,\n",
    "    'optimizer':optimizer,\n",
    "    'best_model_wts_dev_loss':copy.deepcopy(model.state_dict()),\n",
    "    'best_model_wts_test_loss':copy.deepcopy(model.state_dict()),\n",
    "}\n",
    "\n",
    "for key in hyperparameters:\n",
    "    state[key] = hyperparameters[key]\n",
    "\n",
    "experiment_path = f'{TMP_EXPERIMENTS_PATH}/{len(os.listdir(f\"{TMP_EXPERIMENTS_PATH}\")) + 1}'\n",
    "\n",
    "os.makedirs(experiment_path)\n",
    "\n",
    "# TODO : allow for multiple dataloaders\n",
    "# TODO : f1 score each epoch and add to plot_loss\n",
    "# TODO : add best model epoch\n",
    "# TODO : use a torch generator\n",
    "# TODO : remove things after yield in train\n",
    "\n",
    "for state in train(state,**dataloaders):\n",
    "    state['model'].eval()\n",
    "\n",
    "    ## Temporary ##\n",
    "    with torch.no_grad():\n",
    "        loss,y_true,y_pred = evaluate(dataloader=dataloaders['testloader'],model=state['model'],criterion=state['criterion'],device=state['device'])\n",
    "        state['testlossi'].append(loss)\n",
    "        state['testf1i'].append(f1_score(y_true,y_pred,average='macro'))\n",
    "        \n",
    "        if state['testlossi'][-1] < state['best_test_loss']:\n",
    "            state['best_test_loss'] = state['testlossi'][-1]\n",
    "            state['best_test_loss_epoch'] = len(state['testlossi'])-1\n",
    "            state['best_model_wts_test_loss'] = copy.deepcopy(state['model'].state_dict())\n",
    "\n",
    "    ####################\n",
    "    \n",
    "    plot_loss(state,experiment_path)\n",
    "    torch.save(state, f'{experiment_path}/state.pt')\n",
    "torch.save(state, f'{experiment_path}/state.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
